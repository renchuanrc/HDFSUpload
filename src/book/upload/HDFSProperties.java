package book.upload;

public class HDFSProperties {
//	public static String ip = "192.168.2.72";
//	public static  int port = 54310;
//	public static String HADOOP_HOME = "/hadoop-1.2.1";
//	public static String user = "root";
//	public static String passwd = "123456";
//	
//	//读取节点信息属性
//	public static String HADOOP_DATA_DIR = "/hadoopSpace/dfs/data/current/";
//	public static String dataNodeUser = "root";
//	public static String dataNodePasswd = "123456";
	
	public static String ip = "10.61.2.17";
	public static  int port = 54310;
	public static String HADOOP_HOME = "/root/hadoop/hadoop-1.2.1";
	public static String user = "root";
	public static String passwd = "sigsit.cc.org";
	
	//读取节点信息属性
	public static String HADOOP_DATA_DIR = "/hadoopSpace/dfs/data/current/";
	public static String dataNodeUser = "root";
	public static String dataNodePasswd = "sigsit.cc.org";
//	
//	//namenode节点信息
//	public static String ip = "10.17.23.128";
//	public static  int port = 9000;
//	public static String HADOOP_HOME = "/hadoop";
//	public static String user = "root";
//	public static String passwd = "7581293";
//	
//	//读取datanode节点信息属性
//	public static String HADOOP_DATA_DIR = "/hadoopSpace/dfs/data/current/";
//	public static String dataNodeUser = "root";
//	public static String dataNodePasswd = "7581293";
	
	
	//测试用 无hadoop
	public static boolean NonHadoop = false;
	public static String localtem="C:/tmp/";
}
